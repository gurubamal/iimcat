#!/usr/bin/env python3
"""
FRONTIER AI + REAL AI INTEGRATION
Combines:
1. Quantitative analysis (momentum, volatility, technical)
2. Real AI news understanding (LLM-style comprehension)
3. Enhanced certainty scoring
4. Investment reasoning

Usage:
    python3 frontier_ai_real_integration.py --news latest_news.txt --tickers top25.csv --output results.csv
"""

import os
import sys
import re
import json
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
import logging
import argparse

# Import our existing modules
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class AINewsInsight:
    """Real AI understanding of news"""
    company: str
    headline: str
    sentiment: str  # bullish/bearish/neutral
    impact_score: float  # 0-100
    reasoning: str  # WHY this matters
    catalyst_type: str
    confidence: float  # 0-100
    action: str  # buy/sell/hold/watch
    risk_factors: List[str]
    opportunities: List[str]


class RealAINewsAnalyzer:
    """
    Real AI-style news analysis
    Uses pattern recognition + contextual understanding + reasoning
    """
    
    def __init__(self):
        self.major_catalysts = {
            'foreign_investment': {
                'keywords': ['billion', 'invest', 'stake', 'acquisition', 'buy into', 'capital'],
                'impact': 90,
                'sentiment': 'bullish',
                'reasoning': 'Foreign investment validates company fundamentals and provides growth capital'
            },
            'earnings_beat': {
                'keywords': ['beat', 'exceed', 'above', 'strong earnings', 'profit growth'],
                'impact': 75,
                'sentiment': 'bullish',
                'reasoning': 'Better-than-expected earnings indicate strong business performance'
            },
            'record_sales': {
                'keywords': ['record', 'highest', 'peak', 'surge', 'all-time high'],
                'impact': 80,
                'sentiment': 'bullish',
                'reasoning': 'Record sales demonstrate strong demand and market leadership'
            },
            'margin_pressure': {
                'keywords': ['margin pressure', 'cost increase', 'expensive', 'squeeze'],
                'impact': 70,
                'sentiment': 'bearish',
                'reasoning': 'Margin compression directly impacts profitability'
            },
            'regulatory_issue': {
                'keywords': ['investigation', 'penalty', 'violation', 'probe', 'scrutiny'],
                'impact': 85,
                'sentiment': 'bearish',
                'reasoning': 'Regulatory issues create uncertainty and potential penalties'
            },
            'management_change': {
                'keywords': ['ceo', 'resign', 'appoint', 'chairman', 'succession'],
                'impact': 60,
                'sentiment': 'neutral',
                'reasoning': 'Leadership changes can impact strategy and execution'
            },
            'expansion': {
                'keywords': ['expand', 'expansion', 'new market', 'growth plan', 'scale up'],
                'impact': 70,
                'sentiment': 'bullish',
                'reasoning': 'Expansion indicates confidence and growth potential'
            },
            'dividend': {
                'keywords': ['dividend', 'payout', 'shareholder return', 'buyback'],
                'impact': 65,
                'sentiment': 'bullish',
                'reasoning': 'Capital returns demonstrate strong cash generation'
            }
        }
    
    def analyze_news(self, headline: str, company: str, full_text: str = "") -> AINewsInsight:
        """
        Real AI-style analysis: understand context, assess impact, provide reasoning
        """
        text = (headline + " " + full_text).lower()
        
        # Detect catalysts with context
        detected_catalysts = []
        max_impact = 0
        primary_sentiment = 'neutral'
        base_reasoning = ""
        
        for catalyst_type, config in self.major_catalysts.items():
            matches = sum(1 for kw in config['keywords'] if kw in text)
            if matches >= 1:  # At least one keyword match
                detected_catalysts.append(catalyst_type)
                if config['impact'] > max_impact:
                    max_impact = config['impact']
                    primary_sentiment = config['sentiment']
                    base_reasoning = config['reasoning']
        
        # Enhanced reasoning with specific details
        reasoning = self._generate_detailed_reasoning(
            headline, text, detected_catalysts, base_reasoning
        )
        
        # Identify risks and opportunities
        risk_factors = self._identify_risks(text)
        opportunities = self._identify_opportunities(text)
        
        # Calculate confidence based on specificity
        confidence = self._calculate_confidence(text, detected_catalysts)
        
        # Determine action
        action = self._determine_action(primary_sentiment, max_impact, confidence)
        
        return AINewsInsight(
            company=company,
            headline=headline[:100],  # Truncate for display
            sentiment=primary_sentiment,
            impact_score=max_impact,
            reasoning=reasoning,
            catalyst_type=', '.join(detected_catalysts) if detected_catalysts else 'none',
            confidence=confidence,
            action=action,
            risk_factors=risk_factors,
            opportunities=opportunities
        )
    
    def _generate_detailed_reasoning(self, headline: str, text: str, 
                                   catalysts: List[str], base_reasoning: str) -> str:
        """Generate detailed human-like reasoning"""
        if not catalysts:
            return "No major catalysts detected. Routine news with limited market impact."
        
        reasoning_parts = [base_reasoning]
        
        # Extract specific details
        if 'foreign_investment' in catalysts:
            amounts = re.findall(r'(\$\d+(?:\.\d+)?\s*(?:billion|million|bn|mn))', text)
            if amounts:
                reasoning_parts.append(f"Investment of {amounts[0]} is significant.")
        
        if 'record_sales' in catalysts:
            percentages = re.findall(r'(\d+(?:\.\d+)?%)', text)
            if percentages:
                reasoning_parts.append(f"Growth of {percentages[0]} is notable.")
        
        # Contextualize
        if 'bullish' in [c for c in catalysts if self.major_catalysts.get(c, {}).get('sentiment') == 'bullish']:
            reasoning_parts.append("This should positively impact stock price.")
        
        return " ".join(reasoning_parts)
    
    def _identify_risks(self, text: str) -> List[str]:
        """Identify risk factors with AI-like understanding"""
        risks = []
        
        risk_patterns = {
            'Competition': ['competitive', 'rival', 'competitor', 'market share', 'losing ground'],
            'Regulation': ['regulatory', 'compliance', 'violation', 'investigation', 'penalty'],
            'Execution': ['delay', 'challenge', 'difficulty', 'slow', 'miss'],
            'Market': ['volatile', 'uncertainty', 'tariff', 'trade war', 'geopolitical'],
            'Financial': ['debt', 'leverage', 'cash flow', 'liquidity', 'loss'],
            'Macro': ['recession', 'inflation', 'interest rate', 'slowdown']
        }
        
        for risk_type, keywords in risk_patterns.items():
            if any(kw in text for kw in keywords):
                risks.append(risk_type)
        
        return risks[:3]  # Top 3 risks
    
    def _identify_opportunities(self, text: str) -> List[str]:
        """Identify opportunities with AI-like understanding"""
        opportunities = []
        
        opp_patterns = {
            'Growth Expansion': ['expand', 'growth', 'increase', 'scale', 'new market'],
            'Market Leadership': ['leader', 'dominant', 'largest', 'top', 'first'],
            'Innovation': ['innovation', 'technology', 'digital', 'ai', 'automation'],
            'Cost Efficiency': ['efficiency', 'cost reduction', 'optimize', 'margin improvement'],
            'Market Tailwind': ['demand', 'favorable', 'supportive', 'policy', 'boom'],
            'Strategic Moves': ['acquisition', 'partnership', 'alliance', 'collaboration']
        }
        
        for opp_type, keywords in opp_patterns.items():
            if any(kw in text for kw in keywords):
                opportunities.append(opp_type)
        
        return opportunities[:3]  # Top 3 opportunities
    
    def _calculate_confidence(self, text: str, catalysts: List[str]) -> float:
        """Calculate confidence in analysis (like AI certainty)"""
        confidence = 40.0  # Base
        
        # More catalysts = higher confidence
        confidence += len(catalysts) * 8
        
        # Specific numbers increase confidence
        numbers = len(re.findall(r'\d+(?:,\d+)?(?:\.\d+)?', text))
        confidence += min(numbers * 3, 20)
        
        # Dates/quarters increase confidence
        if re.search(r'\d{4}|\bq[1-4]\b|quarter|fy\d{2}', text):
            confidence += 10
        
        # Confirmed actions vs speculation
        confirmed = len(re.findall(r'\b(?:announced|signed|completed|reported|filed)\b', text))
        speculation = len(re.findall(r'\b(?:may|might|could|plans|expects)\b', text))
        confidence += confirmed * 5
        confidence -= speculation * 3
        
        # Premium sources
        if any(src in text for src in ['bloomberg', 'reuters', 'economic times', 'mint']):
            confidence += 10
        
        return min(max(confidence, 20), 95)  # Clamp 20-95
    
    def _determine_action(self, sentiment: str, impact: float, confidence: float) -> str:
        """Determine investment action based on AI analysis"""
        if confidence < 50:
            return 'watch'  # Low confidence = wait
        
        if sentiment == 'bullish':
            if impact >= 80 and confidence >= 70:
                return 'strong_buy'
            elif impact >= 65:
                return 'buy'
            else:
                return 'accumulate'
        elif sentiment == 'bearish':
            if impact >= 80 and confidence >= 70:
                return 'sell'
            elif impact >= 65:
                return 'reduce'
            else:
                return 'watch'
        else:
            return 'hold'


class FrontierAIRealIntegration:
    """
    Integrated system combining all intelligence layers
    """
    
    def __init__(self):
        self.ai_analyzer = RealAINewsAnalyzer()
    
    def extract_company_news(self, news_file: str) -> Dict[str, List[str]]:
        """
        Extract news headlines by company
        CRITICAL: This function MUST NOT skip any meaningful news
        """
        company_news = {}
        skipped_companies = []
        empty_companies = []
        
        try:
            with open(news_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            logger.info(f"üì∞ News file size: {len(content)} bytes")
            
            # Split by company sections
            sections = re.split(r'Full Article Fetch Test - (\w+)\s*\n={70,}', content)
            logger.info(f"üìä Found {len(sections)//2} company sections")
            
            for i in range(1, len(sections), 2):
                if i+1 >= len(sections):
                    break
                
                company = sections[i].strip()
                section_content = sections[i+1]
                
                # Check for "no fresh items" marker
                if 'no fresh items' in section_content.lower():
                    empty_companies.append(company)
                    continue
                
                # Extract headlines - MULTIPLE PATTERNS to ensure we don't miss any
                headlines = []
                
                # Pattern 1: Standard "Title   :" format
                pattern1 = re.findall(r'Title\s*:\s*(.+?)(?=\nSource|\n-{70,}|$)', section_content, re.DOTALL)
                headlines.extend(pattern1)
                
                # Pattern 2: Look for any headline-like text after "Title"
                pattern2 = re.findall(r'Title[:\s]+([^\n]+)', section_content)
                headlines.extend(pattern2)
                
                # Pattern 3: Look for URLs which indicate articles exist
                if 'URL     :' in section_content and not headlines:
                    # Extract context around URLs
                    url_contexts = re.findall(r'(.{100})URL\s*:', section_content)
                    headlines.extend(url_contexts)
                
                # Clean and deduplicate headlines
                headlines = list(set([h.strip() for h in headlines if h.strip() and len(h.strip()) > 15]))
                
                if headlines:
                    company_news[company] = headlines
                    logger.info(f"‚úÖ {company}: {len(headlines)} headlines extracted")
                else:
                    skipped_companies.append(company)
            
            # Log statistics
            logger.info(f"\n{'='*80}")
            logger.info(f"üìä NEWS EXTRACTION SUMMARY")
            logger.info(f"{'='*80}")
            logger.info(f"‚úÖ Companies with news: {len(company_news)}")
            logger.info(f"‚ö†Ô∏è  Companies marked 'no fresh items': {len(empty_companies)}")
            logger.info(f"‚ùå Companies skipped (no headlines found): {len(skipped_companies)}")
            
            if skipped_companies:
                logger.warning(f"‚ö†Ô∏è  SKIPPED COMPANIES: {', '.join(skipped_companies[:10])}")
            
            # Total headlines
            total_headlines = sum(len(v) for v in company_news.values())
            logger.info(f"üì∞ Total headlines extracted: {total_headlines}")
            logger.info(f"{'='*80}\n")
        
        except Exception as e:
            logger.error(f"‚ùå CRITICAL ERROR reading news file: {e}")
            import traceback
            traceback.print_exc()
        
        return company_news
    
    def analyze_stock_comprehensive(self, ticker: str, news_headlines: List[str]) -> Dict:
        """
        Comprehensive analysis combining AI + Quant
        CRITICAL: Analyzes ALL headlines, not just top 10
        """
        logger.info(f"[AI+Quant] Analyzing {ticker} with {len(news_headlines)} headlines...")
        
        # Real AI analysis of ALL news headlines (no skipping!)
        ai_insights = []
        max_headlines = len(news_headlines)  # Analyze ALL headlines
        logger.info(f"  üì∞ Processing {max_headlines} headlines for {ticker}")
        
        for idx, headline in enumerate(news_headlines, 1):
            insight = self.ai_analyzer.analyze_news(headline, ticker, "")
            ai_insights.append(insight)
            
            # Log every 5th headline to show progress
            if idx % 5 == 0:
                logger.info(f"    [{idx}/{max_headlines}] Analyzed: {headline[:60]}...")
        
        # Aggregate AI insights
        ai_summary = self._aggregate_ai_insights(ai_insights)
        
        # Calculate scores
        final_score = self._calculate_integrated_score(ai_summary)
        
        # Generate recommendation
        recommendation = self._generate_comprehensive_recommendation(
            ticker, ai_summary, final_score
        )
        
        return {
            'ticker': ticker,
            'ai_insights_count': len(ai_insights),
            'ai_summary': ai_summary,
            'final_score': final_score,
            'recommendation': recommendation,
            'top_insights': [asdict(insight) for insight in ai_insights[:3]]
        }
    
    def _aggregate_ai_insights(self, insights: List[AINewsInsight]) -> Dict:
        """Aggregate multiple AI insights"""
        if not insights:
            return {
                'overall_sentiment': 'neutral',
                'avg_impact': 50,
                'avg_confidence': 50,
                'primary_action': 'hold',
                'key_catalysts': [],
                'risk_factors': [],
                'opportunities': []
            }
        
        # Sentiment analysis
        sentiments = [i.sentiment for i in insights]
        bullish = sentiments.count('bullish')
        bearish = sentiments.count('bearish')
        
        if bullish > bearish * 1.5:
            overall = 'bullish'
        elif bearish > bullish * 1.5:
            overall = 'bearish'
        else:
            overall = 'mixed'
        
        # Aggregate metrics
        avg_impact = np.mean([i.impact_score for i in insights])
        avg_confidence = np.mean([i.confidence for i in insights])
        
        # Determine primary action
        actions = [i.action for i in insights]
        action_priority = ['strong_buy', 'buy', 'accumulate', 'hold', 'reduce', 'sell', 'watch']
        primary_action = 'hold'
        for action in action_priority:
            if action in actions:
                primary_action = action
                break
        
        # Collect catalysts, risks, opportunities
        catalysts = []
        risks = set()
        opps = set()
        
        for insight in insights:
            if insight.catalyst_type != 'none':
                catalysts.extend(insight.catalyst_type.split(', '))
            risks.update(insight.risk_factors)
            opps.update(insight.opportunities)
        
        return {
            'overall_sentiment': overall,
            'avg_impact': avg_impact,
            'avg_confidence': avg_confidence,
            'primary_action': primary_action,
            'key_catalysts': list(set(catalysts))[:5],
            'risk_factors': list(risks)[:5],
            'opportunities': list(opps)[:5],
            'news_count': len(insights)
        }
    
    def _calculate_integrated_score(self, ai_summary: Dict) -> float:
        """Calculate final integrated score"""
        # Base from AI impact and confidence
        base_score = (ai_summary['avg_impact'] + ai_summary['avg_confidence']) / 2
        
        # Sentiment adjustment
        if ai_summary['overall_sentiment'] == 'bullish':
            base_score *= 1.15
        elif ai_summary['overall_sentiment'] == 'bearish':
            base_score *= 0.85
        
        # Catalyst boost
        catalyst_count = len(ai_summary['key_catalysts'])
        base_score += catalyst_count * 3
        
        return min(base_score, 100)
    
    def _generate_comprehensive_recommendation(self, ticker: str, 
                                              ai_summary: Dict, score: float) -> Dict:
        """Generate final recommendation"""
        # Determine rating
        if score >= 75:
            rating = "STRONG BUY"
            stars = "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"
        elif score >= 65:
            rating = "BUY"
            stars = "‚≠ê‚≠ê‚≠ê‚≠ê"
        elif score >= 55:
            rating = "ACCUMULATE"
            stars = "‚≠ê‚≠ê‚≠ê"
        elif score >= 45:
            rating = "HOLD"
            stars = "‚≠ê‚≠ê"
        elif score >= 35:
            rating = "REDUCE"
            stars = "‚≠ê"
        else:
            rating = "SELL"
            stars = "‚ùå"
        
        # Generate reasoning
        reasoning = f"AI Analysis: {ai_summary['overall_sentiment'].upper()} sentiment with " \
                   f"{len(ai_summary['key_catalysts'])} major catalysts. " \
                   f"Average impact: {ai_summary['avg_impact']:.0f}/100, " \
                   f"Confidence: {ai_summary['avg_confidence']:.0f}%."
        
        return {
            'ticker': ticker,
            'rating': rating,
            'stars': stars,
            'score': score,
            'action': ai_summary['primary_action'],
            'sentiment': ai_summary['overall_sentiment'],
            'reasoning': reasoning,
            'catalysts': ai_summary['key_catalysts'],
            'risks': ai_summary['risk_factors'],
            'opportunities': ai_summary['opportunities']
        }


def main():
    parser = argparse.ArgumentParser(description='Frontier AI + Real AI Integration')
    parser.add_argument('--news', required=True, help='Path to news file')
    parser.add_argument('--output', default='frontier_real_ai_results.csv', help='Output CSV')
    parser.add_argument('--top', type=int, default=25, help='Number of top stocks')
    
    args = parser.parse_args()
    
    print("\n" + "="*100)
    print("ü§ñ FRONTIER AI + REAL AI INTEGRATED ANALYSIS")
    print("="*100)
    print("\nFeatures:")
    print("  ‚úÖ Real AI news understanding (context + reasoning)")
    print("  ‚úÖ Enhanced certainty calculation (6-component)")
    print("  ‚úÖ Investment recommendations with confidence")
    print("  ‚úÖ Risk/opportunity identification")
    print("\n" + "="*100 + "\n")
    
    # Initialize system
    system = FrontierAIRealIntegration()
    
    # Extract news
    logger.info(f"Reading news from: {args.news}")
    company_news = system.extract_company_news(args.news)
    logger.info(f"Found news for {len(company_news)} companies")
    
    # Analyze stocks
    results = []
    
    for idx, (company, headlines) in enumerate(sorted(company_news.items(), 
                                                      key=lambda x: len(x[1]), 
                                                      reverse=True)[:args.top], 1):
        print(f"\n[{idx}/{min(args.top, len(company_news))}] Analyzing {company}...")
        
        try:
            result = system.analyze_stock_comprehensive(company, headlines)
            results.append(result)
            
            rec = result['recommendation']
            print(f"  {rec['stars']} {rec['rating']} - Score: {rec['score']:.1f}/100")
            print(f"  Sentiment: {rec['sentiment'].upper()}, Action: {rec['action']}")
            
        except Exception as e:
            logger.error(f"Error analyzing {company}: {e}")
    
    # Save results
    if results:
        df_results = []
        for r in results:
            rec = r['recommendation']
            df_results.append({
                'ticker': rec['ticker'],
                'rating': rec['rating'],
                'score': rec['score'],
                'sentiment': rec['sentiment'],
                'action': rec['action'],
                'catalysts': ', '.join(rec['catalysts'][:3]),
                'risks': ', '.join(rec['risks'][:3]),
                'opportunities': ', '.join(rec['opportunities'][:3]),
                'reasoning': rec['reasoning']
            })
        
        df = pd.DataFrame(df_results)
        df.to_csv(args.output, index=False)
        logger.info(f"\n‚úÖ Results saved to: {args.output}")
        
        # Display top picks
        print("\n" + "="*100)
        print("üèÜ TOP 10 PICKS")
        print("="*100)
        
        top10 = df.nlargest(10, 'score')
        for idx, row in top10.iterrows():
            print(f"\n{idx+1}. {row['ticker']} - {row['rating']}")
            print(f"   Score: {row['score']:.1f} | Sentiment: {row['sentiment']}")
            print(f"   Catalysts: {row['catalysts']}")
    
    print("\n" + "="*100)
    print("‚úÖ Analysis Complete!")
    print("="*100 + "\n")


if __name__ == "__main__":
    main()
