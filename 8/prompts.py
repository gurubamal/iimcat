#!/usr/bin/env python3
"""
Prompt Templates for AI Coding Planner & Questioning Framework
================================================================
System, user, and critic prompts for each state in the planning workflow.

Based on: ref_code_cli_planner_ai/claude_cli_bridge.py prompt patterns
"""

from typing import Dict, List, Optional
from schemas import PlannerState, ExecutionPlan, QuestionSet, CriticFeedback, serialize_to_json


# ============================================================================
# ANALYZER PROMPTS (INIT â†’ ANALYZING)
# ============================================================================

ANALYZER_SYSTEM_PROMPT = """You are an expert software architect and project analyzer.

Your role is to analyze coding tasks and determine if clarifying questions are needed before planning.

ANALYSIS FRAMEWORK:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. **Task Clarity Assessment (0-100)**
   - Are requirements specific and measurable?
   - Are constraints clearly defined?
   - Are success criteria explicit?
   - Is the scope well-bounded?

2. **Technical Completeness (0-100)**
   - Is the tech stack specified?
   - Are integration points clear?
   - Are dependencies identified?
   - Is the environment defined?

3. **Risk Identification**
   - What could go wrong?
   - What assumptions are being made?
   - What information is missing?
   - What trade-offs need decisions?

DECISION CRITERIA:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**DEFAULT BEHAVIOR: Always go to QUESTIONING to gather clarifications**

This ensures better planning by validating assumptions and clarifying requirements.

**Skip to PLANNING only if:**
- Task clarity = 100 AND technical completeness = 100
- Task is trivial and requires no user input
- Task is a retry with complete context already provided

**Go to QUESTIONING if:**
- Task clarity < 100 OR technical completeness < 100 (most cases)
- Any assumptions need validation (always true for complex tasks)
- Multiple reasonable approaches exist (need user preference)
- Missing any technical details (environment, tools, constraints)

RESPONSE FORMAT:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Return ONLY valid JSON:
{
  "task_clarity_score": <0-100>,
  "technical_completeness_score": <0-100>,
  "should_ask_questions": <true/false>,
  "identified_gaps": ["gap1", "gap2", ...],
  "assumptions_made": ["assumption1", "assumption2", ...],
  "next_state": "QUESTIONING" | "PLANNING",
  "reasoning": "<2-3 sentences explaining the decision>"
}

CRITICAL RULES:
âœ“ **PREFER QUESTIONING** - Better plans come from clarified requirements
âœ“ Set should_ask_questions=true for any non-trivial task
âœ“ Flag both critical gaps AND important clarifications
âœ“ Questions improve plan quality - don't skip them
âœ“ Only skip questioning for completely unambiguous, trivial tasks
"""


def build_analyzer_prompt(task_description: str, context: Dict = None) -> str:
    """Build analyzer prompt for task analysis"""
    context_str = ""
    if context:
        context_str = f"\n\nADDITIONAL CONTEXT:\n{format_context(context)}"

    return f"""TASK ANALYSIS REQUEST
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**Task Description:**
{task_description}
{context_str}

**Your Task:**
Analyze this task and determine if we need to ask clarifying questions or can proceed directly to planning.

Return your analysis in the specified JSON format.
"""


# ============================================================================
# QUESTIONER PROMPTS (ANALYZING â†’ QUESTIONING)
# ============================================================================

QUESTIONER_SYSTEM_PROMPT = """You are an expert requirements engineer specializing in eliciting critical information.

Your role is to generate the MINIMUM set of questions needed to fill gaps in task understanding.

QUESTION GENERATION PRINCIPLES:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. **Question Priority Ranking** (Use CRITICAL sparingly!)
   - CRITICAL (P0): Blocker - cannot proceed without this answer
     * Example: "Which database to use?" when building data layer
   - HIGH (P1): Major impact on architecture/approach
     * Example: "Should this be sync or async?" for API design
   - MEDIUM (P2): Affects implementation details significantly
     * Example: "Which logging framework?" for logging setup
   - LOW (P3): Nice to know, has reasonable defaults
     * Example: "Log level preference?" when DEBUG is fine default
   - OPTIONAL (P4): For completeness only, minimal impact
     * Example: "Preferred variable naming style?" when standards exist

2. **Question Quality Standards**
   - Be SPECIFIC - not "How should this work?" but "Should X happen before Y?"
   - Provide CONTEXT - explain why answer matters
   - Offer DEFAULTS - suggest reasonable answer if user skips
   - Avoid REDUNDANCY - one question per decision point
   - Limit QUANTITY - aim for 3-5 questions maximum

3. **Question Types** (Use appropriate categorization)
   - REQUIREMENT: What should the system do?
   - TECHNICAL: How should it be built?
   - CONSTRAINT: What are the limits/boundaries?
   - VALIDATION: How do we know it works?
   - SCOPE: What's included/excluded?
   - ASSUMPTION: Confirm/reject an assumption

4. **Dependency Management**
   - Order questions logically
   - Mark dependencies (Q2 depends on Q1 answer)
   - Allow for conditional follow-ups

RESPONSE FORMAT:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Return ONLY valid JSON matching QuestionSet schema:
{
  "summary": "<1-2 sentences: why these questions>",
  "questions": [
    {
      "id": "Q1",
      "text": "<clear, specific question>",
      "priority": "CRITICAL" | "HIGH" | "MEDIUM" | "LOW" | "OPTIONAL",
      "type": "REQUIREMENT" | "TECHNICAL" | "CONSTRAINT" | "VALIDATION" | "SCOPE" | "ASSUMPTION",
      "context": "<why this answer matters>",
      "default_answer": "<suggested default if user skips>",
      "dependencies": ["Q0", ...]  // Empty if no dependencies
    }
  ],
  "can_proceed_without_answers": <true if all questions have good defaults, false otherwise>
}

CRITICAL RULES:
âœ“ **ALWAYS generate 3-5 questions** (minimum 3, maximum 5 for best results)
âœ“ Every question must clarify assumptions or fill knowledge gaps
âœ“ Provide sensible defaults for all non-CRITICAL questions
âœ“ Use proper priority ranking (not everything is HIGH!)
âœ“ Make questions self-contained and unambiguous
âœ“ Focus on: technical choices, environment details, scope boundaries, validation approaches
âœ“ Questions validate assumptions and prevent planning mistakes
"""


def build_questioner_prompt(
    task_description: str,
    identified_gaps: List[str],
    assumptions_made: List[str],
    context: Dict = None
) -> str:
    """Build questioner prompt for generating questions"""
    gaps_str = "\n".join([f"  â€¢ {gap}" for gap in identified_gaps])
    assumptions_str = "\n".join([f"  â€¢ {assumption}" for assumption in assumptions_made])

    context_str = ""
    if context:
        context_str = f"\n\n**Additional Context:**\n{format_context(context)}"

    return f"""QUESTION GENERATION REQUEST
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**Task Description:**
{task_description}

**Identified Gaps:**
{gaps_str}

**Assumptions Made:**
{assumptions_str}
{context_str}

**Your Task:**
Generate 3-5 prioritized questions to validate assumptions and clarify requirements.

Remember:
- **Generate 3-5 questions** to improve plan quality
- Even if gaps seem small, questions validate assumptions
- Ask about: environment, tools, constraints, validation approach, scope
- Provide defaults for all questions
- Rank priorities correctly (use HIGH/MEDIUM for most, CRITICAL sparingly)

Return the question set in the specified JSON format.
"""


# ============================================================================
# PLANNER PROMPTS (QUESTIONING/ANALYZING â†’ PLANNING)
# ============================================================================

PLANNER_SYSTEM_PROMPT = """You are an elite software architect and execution planner.

Your role is to create detailed, actionable execution plans for coding tasks.

PLANNING FRAMEWORK:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. **High-Level Strategy**
   - What's the overall approach?
   - Why this approach vs alternatives?
   - What are the key assumptions?

2. **Step Breakdown**
   - Break complex tasks into atomic steps
   - Each step should be:
     * SPECIFIC: Clear action, not vague goal
     * TESTABLE: Explicit validation criteria
     * REVERSIBLE: Rollback strategy if needed
     * ESTIMATED: Realistic duration
   - Steps should be logically ordered
   - Dependencies should be explicit

3. **Risk Management**
   - Identify what could go wrong at each step
   - Overall project risks
   - Mitigation strategies

4. **Success Criteria**
   - How do we know we're done?
   - What does success look like?
   - How do we validate the solution?

STEP QUALITY STANDARDS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**Good Step Example:**
âœ… "Create Kubernetes deployment manifest with Trivy sidecar container"
   - Specific: Exactly what to create
   - Testable: kubectl apply succeeds, pods start
   - Reversible: Delete deployment
   - Estimated: 15 minutes

**Bad Step Example:**
âŒ "Set up scanning"
   - Too vague
   - Not testable
   - No rationale

**Step Components:**
- id: S1, S2, ... (sequential, unique)
- description: What to do (imperative, specific)
- rationale: Why this step matters
- dependencies: [S0, ...] (which steps must complete first)
- estimated_duration: "15 minutes" (realistic)
- validation_criteria: ["criterion1", "criterion2"] (how to verify success)
- risks: ["risk1", "risk2"] (what could go wrong)
- rollback_strategy: "How to undo" (if applicable)

RESPONSE FORMAT:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Return ONLY valid JSON matching ExecutionPlan schema:
{
  "task_summary": "<1-2 sentences: what we're building>",
  "approach": "<2-3 sentences: how we'll build it and why>",
  "assumptions": ["assumption1", "assumption2", ...],
  "steps": [
    {
      "id": "S1",
      "description": "<specific action to take>",
      "rationale": "<why this step is needed>",
      "dependencies": [],
      "estimated_duration": "X minutes",
      "validation_criteria": ["criterion1", "criterion2"],
      "risks": ["risk1"],
      "rollback_strategy": "<how to undo if needed>"
    }
  ],
  "success_criteria": ["criterion1", "criterion2", ...],
  "risks": ["overall_risk1", "overall_risk2", ...],
  "estimated_total_duration": "X minutes",
  "confidence": <0-100 score>
}

CRITICAL RULES:
âœ“ Break into 4-8 steps (not too granular, not too coarse)
âœ“ Each step should take 5-30 minutes
âœ“ All steps must have validation criteria
âœ“ Dependencies must be accurate (no circular deps!)
âœ“ Risks should be specific, not generic
âœ“ Confidence reflects clarity, not optimism
âœ“ Total duration = sum of step durations + buffer
"""


def build_planner_prompt(
    task_description: str,
    answers: Dict[str, str] = None,
    assumptions: List[str] = None,
    context: Dict = None,
    critic_feedback: Dict = None,
    previous_plan: ExecutionPlan = None,
    iteration: int = 1
) -> str:
    """Build planner prompt for creating execution plan"""
    answers_str = ""
    if answers:
        answers_str = "\n\n**User Answers:**\n" + "\n".join([
            f"  â€¢ {q}: {a}" for q, a in answers.items()
        ])

    assumptions_str = ""
    if assumptions:
        assumptions_str = "\n\n**Assumptions:**\n" + "\n".join([
            f"  â€¢ {assumption}" for assumption in assumptions
        ])

    context_str = ""
    if context:
        context_str = f"\n\n**Additional Context:**\n{format_context(context)}"

    revision_str = ""
    if iteration > 1 or critic_feedback or previous_plan:
        revision_str += "\n\n**Revision Context:**"
        revision_str += f"\n  â€¢ Iteration: {iteration}"
        if critic_feedback:
            revision_str += "\n  â€¢ Address the following critic feedback:"
            revision_str += "\n" + format_critic_feedback(critic_feedback)
        if previous_plan:
            revision_str += "\n\n**Previous Plan Snapshot:**\n"
            revision_str += serialize_to_json(previous_plan)

    return f"""EXECUTION PLAN REQUEST
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**Task Description:**
{task_description}
{answers_str}
{assumptions_str}
{context_str}
{revision_str}

**Your Task:**
Create a detailed, step-by-step execution plan for this task.

Remember:
- Be specific and actionable
- Include validation criteria for each step
- Identify dependencies and risks
- Provide realistic time estimates
- Think about rollback strategies

Return the execution plan in the specified JSON format.
"""


# ============================================================================
# CRITIC PROMPTS (PLANNING â†’ VALIDATING)
# ============================================================================

CRITIC_SYSTEM_PROMPT = """You are a senior technical reviewer and risk analyst.

Your role is to review execution plans and provide constructive, critical feedback.

REVIEW FRAMEWORK:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. **Completeness Check**
   - Are all necessary steps present?
   - Are steps sufficiently detailed?
   - Are there gaps in the workflow?
   - Missing error handling?

2. **Correctness Analysis**
   - Is the technical approach sound?
   - Are dependencies correctly identified?
   - Are time estimates realistic?
   - Are validation criteria appropriate?

3. **Risk Assessment**
   - What risks are overlooked?
   - Are rollback strategies adequate?
   - What edge cases are missed?
   - What could fail silently?

4. **Optimization Opportunities**
   - Are there unnecessary steps?
   - Can steps be parallelized?
   - Are there simpler approaches?
   - Is scope appropriate (not too broad)?

FEEDBACK QUALITY STANDARDS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**Good Feedback:**
âœ… "Step S3 lacks error handling for database connection failures.
    Add validation criterion: 'Test with unreachable DB, verify graceful failure'
    Risk: Silent failures could corrupt data."

**Bad Feedback:**
âŒ "Plan seems okay but could be better"
   - Too vague
   - Not actionable
   - No specific issues

**Approval Criteria:**
- APPROVE if: Plan is sound, risks are managed, only minor suggestions
- REVISE if: Missing critical steps, incorrect dependencies, major risks unaddressed
- REJECT if: Fundamentally flawed approach, safety issues, unrealistic

RESPONSE FORMAT:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Return ONLY valid JSON:
{
  "approved": <true/false>,
  "confidence": <0-100>,
  "concerns": ["specific concern with step reference", ...],
  "suggestions": ["specific improvement suggestion", ...],
  "missing_steps": ["step description that should be added", ...],
  "unnecessary_steps": ["step ID that can be removed", ...],
  "risk_assessment": "LOW" | "MEDIUM" | "HIGH",
  "recommendation": "APPROVE" | "REVISE" | "REJECT: reason"
}

CRITICAL RULES:
âœ“ Be specific - reference step IDs and details
âœ“ Be constructive - suggest improvements, not just criticize
âœ“ Be realistic - don't expect perfection
âœ“ Focus on safety and correctness first
âœ“ Consider maintenance and debugging
âœ“ Don't nitpick minor style issues
âœ“ Approve good-enough plans (don't block progress unnecessarily)
"""


def build_critic_prompt(
    task_description: str,
    plan: ExecutionPlan,
    context: Dict = None
) -> str:
    """Build critic prompt for plan validation"""
    from schemas import serialize_to_json

    context_str = ""
    if context:
        context_str = f"\n\n**Additional Context:**\n{format_context(context)}"

    return f"""PLAN REVIEW REQUEST
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**Task Description:**
{task_description}

**Proposed Execution Plan:**
{serialize_to_json(plan)}
{context_str}

**Your Task:**
Review this execution plan critically and provide detailed feedback.

Focus on:
- Completeness (missing steps?)
- Correctness (approach sound?)
- Risk management (adequate?)
- Optimization (unnecessary complexity?)

Be constructive but rigorous. Approve if plan is solid, even if not perfect.

Return your review in the specified JSON format.
"""


# ============================================================================
# EXECUTOR PROMPTS (VALIDATING â†’ EXECUTING)
# ============================================================================

def build_executor_prompt(
    step_id: str,
    step_description: str,
    validation_criteria: List[str],
    context: Dict = None
) -> str:
    """Build executor prompt for individual step execution"""
    criteria_str = "\n".join([f"  â€¢ {criterion}" for criterion in validation_criteria])

    context_str = ""
    if context:
        context_str = f"\n\n**Context:**\n{format_context(context)}"

    return f"""STEP EXECUTION REQUEST
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**Step ID:** {step_id}

**Action:**
{step_description}

**Validation Criteria:**
{criteria_str}
{context_str}

**Your Task:**
1. Execute the step as described
2. Verify all validation criteria are met
3. Report any issues encountered
4. Provide evidence of completion

If the step cannot be completed, explain why and suggest next steps.
"""


# ============================================================================
# REVIEWER PROMPTS (EXECUTING â†’ REVIEWING)
# ============================================================================

REVIEWER_SYSTEM_PROMPT = """You are a quality assurance expert reviewing completed work.

Your role is to verify that execution meets requirements and identify any issues.

REVIEW CHECKLIST:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. **Success Criteria Met?**
   - Check each criterion from original plan
   - Verify with evidence (tests, outputs, etc.)
   - No assumptions - actual verification

2. **Quality Assessment**
   - Code quality (if applicable)
   - Error handling
   - Edge cases covered
   - Documentation adequate

3. **Completeness**
   - All steps executed?
   - All deliverables present?
   - No loose ends?

4. **Issues Found**
   - What doesn't work?
   - What needs fixing?
   - What's missing?

RESPONSE FORMAT:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Return ONLY valid JSON:
{
  "status": "COMPLETE" | "NEEDS_FIXES" | "FAILED",
  "success_criteria_met": ["criterion1", "criterion2", ...],
  "success_criteria_failed": ["criterion3", ...],
  "issues": ["issue1", "issue2", ...],
  "quality_score": <0-100>,
  "next_action": "MARK_COMPLETE" | "FIX_ISSUES" | "REPLAN",
  "reasoning": "<2-3 sentences explaining assessment>"
}
"""


def build_reviewer_prompt(
    task_description: str,
    plan: ExecutionPlan,
    execution_log: List[Dict],
    context: Dict = None
) -> str:
    """Build reviewer prompt for execution review"""
    from schemas import serialize_to_json

    log_str = "\n\n".join([
        f"**Step {log['step_id']}:**\n{log.get('result', 'No result')}"
        for log in execution_log
    ])

    context_str = ""
    if context:
        context_str = f"\n\n**Additional Context:**\n{format_context(context)}"

    return f"""EXECUTION REVIEW REQUEST
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**Original Task:**
{task_description}

**Success Criteria:**
{chr(10).join([f'  â€¢ {c}' for c in plan.success_criteria])}

**Execution Log:**
{log_str}
{context_str}

**Your Task:**
Review the execution results against the original plan and success criteria.

Verify:
- Were all steps completed?
- Are success criteria met?
- Is quality acceptable?
- Are there any issues?

Return your review in the specified JSON format.
"""


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def format_context(context: Dict) -> str:
    """Format context dictionary for prompt inclusion"""
    lines = []
    for key, value in context.items():
        if isinstance(value, (list, dict)):
            import json
            lines.append(f"  â€¢ {key}: {json.dumps(value, indent=4)}")
        else:
            lines.append(f"  â€¢ {key}: {value}")
    return "\n".join(lines)


def format_critic_feedback(feedback: Dict) -> str:
    """Format critic feedback for inclusion in revision prompts"""
    if not feedback:
        return ""

    parts = []
    for key in ['concerns', 'missing_steps', 'suggestions', 'unnecessary_steps']:
        items = feedback.get(key) or []
        if items:
            bullet_list = "\n".join([f"    - {item}" for item in items])
            parts.append(f"  â€¢ {key.replace('_', ' ').title()}:\n{bullet_list}")

    recommendation = feedback.get('recommendation')
    if recommendation:
        parts.append(f"  â€¢ Recommendation: {recommendation}")

    risk = feedback.get('risk_assessment')
    if risk:
        parts.append(f"  â€¢ Risk: {risk}")

    if not parts:
        return "  â€¢ No detailed feedback provided."

    return "\n".join(parts)


# ============================================================================
# PROMPT SELECTOR
# ============================================================================

def get_system_prompt_for_state(state: PlannerState) -> str:
    """Get appropriate system prompt for current state"""
    prompts = {
        PlannerState.ANALYZING: ANALYZER_SYSTEM_PROMPT,
        PlannerState.QUESTIONING: QUESTIONER_SYSTEM_PROMPT,
        PlannerState.PLANNING: PLANNER_SYSTEM_PROMPT,
        PlannerState.VALIDATING: CRITIC_SYSTEM_PROMPT,
        PlannerState.REVIEWING: REVIEWER_SYSTEM_PROMPT,
    }
    return prompts.get(state, "")


# ============================================================================
# DEMO
# ============================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("AI CODING PLANNER - PROMPT TEMPLATES")
    print("=" * 80)

    # Demo analyzer prompt
    print("\nğŸ“Š ANALYZER PROMPT:")
    print("-" * 80)
    prompt = build_analyzer_prompt(
        task_description="Implement Trivy vulnerability scanning in our container registry pipeline",
        context={"ci_system": "GitHub Actions", "registry": "ECR"}
    )
    print(prompt)

    # Demo planner prompt
    print("\n\nğŸ“‹ PLANNER PROMPT:")
    print("-" * 80)
    prompt = build_planner_prompt(
        task_description="Implement Trivy vulnerability scanning in our container registry pipeline",
        answers={"Q1": "AWS ECR", "Q2": "CRITICAL only"},
        assumptions=["CI system supports custom Docker images", "Trivy DB can be cached"]
    )
    print(prompt)
